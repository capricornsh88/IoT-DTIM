{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f761b76",
   "metadata": {},
   "source": [
    "IoT-DTIM\n",
    "======\n",
    "\n",
    "- This Notebook only contains the functions of device clustering module and deep-learning models.\n",
    "    - It describes the device clustering method and its corresponding functions.\n",
    "    - Also, it explains the structure of each deep learning model and what shape the input and output are.\n",
    "- This code is written in an environment with python 3.6. Also, the version of TensorFlow and Keras are 1.12.0 and 2.2.5, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d4d65d",
   "metadata": {},
   "source": [
    "# Function for training of Device Clustering \n",
    "\n",
    "### Description\n",
    "- This function trains the device clustering model and returns the trained autoencoder and encoder model.\n",
    "- The input data is scaled time-series data with two modalities such as temperature and humidity.\n",
    "- Shape of input data \"X_train\" and \"X_vaild\" is (:, window_size * 2). \n",
    "- The \"X_train[ : , : window_size ]\" is temperature data and The last of \"X_train[ : , window_size : ]\" is humidity data. \n",
    "- The \"X_valid\" is also same. \n",
    "- This function returns the trained autoencoder model and encoder model.\n",
    "- The only encoder model will be used in device_clustering_module.\n",
    "\n",
    "### 1. training_device_clustering()\n",
    "\n",
    "- Input:\n",
    "    - X_train : input and output of autoencoder model for train data in training phase\n",
    "    - X_valid : input and output of autoencoder model for validation data in training phase\n",
    "    - window_size : training window size, this paper set it to 120\n",
    "\n",
    "- Output:\n",
    "    - Rreturns trained autoencoder and encoder model\n",
    "\n",
    "### Example\n",
    "    autoencoder, encoder = training_of_device_clustering(X_train, X_valid, window_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25f03209",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_device_clustering(X_train, X_valid, window_size):\n",
    "\tencoding_dim1 = 128\n",
    "\tencoding_dim2 = 64\n",
    "\tencoding_dim3 = 64\n",
    "\tnb_epoch= 100\n",
    "\tbatch_size = 32\n",
    "\n",
    "\ttarget_sensor = ['temp', 'humi']\n",
    "\tinput_dim = window_size\n",
    "\tprint('input dimension is ', input_dim )\n",
    "\tinput_layer_1= Input (shape = (input_dim,))\n",
    "\tinput_layer_2 = Input (shape = (input_dim,))\n",
    "\n",
    "\ten_L1_1 = Dense (encoding_dim1, activation = \"relu\")(input_layer_1)\n",
    "\ten_L1_2 = Dense (encoding_dim1, activation = \"relu\")(input_layer_2)\n",
    "\ten_L2_1 = Dense (encoding_dim2, activation = \"relu\")(en_L1_1)\n",
    "\ten_L2_2 = Dense (encoding_dim2, activation = \"relu\")(en_L1_2)\n",
    "\tmerge = concatenate([en_L2_1, en_L2_2])\n",
    "\tmerge = Dense (encoding_dim3, activation = 'relu')(merge)\n",
    "\te_L3 = Dense (encoding_dim3, activation = 'relu')(merge)\n",
    "\td_L2_1 = Dense (encoding_dim2, activation = \"relu\")(e_L3)\n",
    "\td_L2_2 = Dense (encoding_dim2, activation = \"relu\")(e_L3)\n",
    "\td_L1_1 = Dense (encoding_dim1, activation = \"relu\")(d_L2_1)\n",
    "\td_L1_2 = Dense (encoding_dim1, activation = \"relu\")(d_L2_2)\n",
    "\td_1_additional = Dense (input_dim, activation = \"relu\")(d_L1_1)\n",
    "\td_2_additional = Dense (input_dim, activation = \"relu\")(d_L1_2)\n",
    "\td_1 = Dense (input_dim, activation = 'linear' )(d_1_additional)\n",
    "\td_2 = Dense (input_dim, activation = 'linear' )(d_2_additional)\n",
    "\n",
    "\tautoencoder = Model (inputs = [input_layer_1, input_layer_2], outputs = [d_1, d_2])\n",
    "\tencoder = Model (inputs = [input_layer_1, input_layer_2], outputs = e_L3)\n",
    "\n",
    "\tautoencoder.summary()\n",
    "\tencoder.summary()\n",
    "\n",
    "\tplot_model(autoencoder, to_file='plot_model/AE.png', show_shapes = True)\n",
    "\tautoencoder.compile(optimizer='adadelta', loss='mean_squared_error', metrics=['accuracy'])\n",
    "\theckpointer = ModelCheckpoint(filepath=\"models/IoT_clustering_AE_model.h5\", verbose=0, save_best_only=True)\n",
    "\ttensorboard = TensorBoard(log_dir='./logs', histogram_freq=0, write_graph=True, write_images=True)\n",
    "\thistory = autoencoder.fit([X_train[:,:window_size], X_train[:,window_size:]], [X_train[:,:window_size], X_train[:,window_size:]],\n",
    "\t                    epochs=nb_epoch,\n",
    "\t                    batch_size=batch_size,\n",
    "\t                    shuffle=True,\n",
    "\t                    validation_data=([X_valid[:,:window_size], X_valid[:,window_size:]], [X_valid[:,:window_size], X_valid[:,window_size:]]),\n",
    "\t                    verbose=2,\n",
    "\t                    callbacks=[checkpointer, tensorboard]).history\n",
    "\tplt.plot(history['loss'])\n",
    "\tplt.plot(history['val_loss'])\n",
    "\tplt.title('model loss')\n",
    "\tplt.ylabel('loss')\n",
    "\tplt.xlabel('epoch')\n",
    "\tplt.legend(['train', 'test'], loc='upper right');\n",
    "\treturn autoencoder, encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00223756",
   "metadata": {},
   "source": [
    "# Device clustering based on trained encoder model and K-means clustering algorithm\n",
    "\n",
    "### Description\n",
    "- The purpose of this procedure is to label the each sensor based on trained encoder model using PCA and K-means clustering algorithm.\n",
    "- First, the \"make_feature_map()\" function is used to extract the features from the time series data of each device through an encoder model.\n",
    "- Next, the dimension of the feature is reduced by using PCA on the extracted features, and the K-means algorithm is applied.\n",
    "- At this time, the \"find_best_nb_of_clustering()\" function selects the optimal number of clusters based on the elbow-point method.\n",
    "- Labeling of each device is performed using a clustering algorithm based on the number of selected clusters.\n",
    "\n",
    "## 1. make_feature_map()\n",
    "- Input:\n",
    "    - encoder : trained encoder model \n",
    "    - X_test_each_sensor_for_clustering : test data of encoder model \n",
    "        - This input is reshaped to the input shape of encoder model \n",
    "\n",
    "- Output:\n",
    "    - feature_each_device : returns the encoded feature for test input data\n",
    "\n",
    "## 2. find_best_nb_of_clustering()\n",
    "- Input:\n",
    "    - feature_each_device : extraced feature from make_feature_map() function \n",
    "    - test_data_index : target test data index\n",
    "\n",
    "- Output:\n",
    "    - best_k : returns the best number of clusters\n",
    "\n",
    "    \n",
    "## 3. clustering()\n",
    "- Input:\n",
    "    - feature_each_device : extraced feature from make_feature_map() function \n",
    "    - km_n_c : best number of clusters results from find_best_nb_of_clustering() function\n",
    "    - test_data_index : target test data index\n",
    "\n",
    "- Output:\n",
    "    - device_clustering_dict : returns the devices cluster labeling results\n",
    "\n",
    "### Example\n",
    "    feature_each_device = make_feature_map(encoder, X_test_each_device_for_clustering)\n",
    "    km_n_c = find_best_nb_of_clustering (feature_each_device, test_data_index)\n",
    "    device_clustering_dict = clustering(feature_each_device, km_n_c, test_data_index)\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aef8c02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################################\n",
    "##################################################################################################\n",
    "def make_feature_map(encoder,X_test_each_sensor_for_clustering):\n",
    "    input_data = X_test_each_sensor_for_clustering\n",
    "    clusterig_window_size =  input_data['temp'].shape[1]\n",
    "    nb_devices = input_data['temp'].shape[2]\n",
    "    X_test_temp = np.transpose(input_data['temp'], (0,2,1)).reshape(-1, clusterig_window_size)\n",
    "    X_test_humi = np.transpose(input_data['humi'], (0,2,1)).reshape(-1, clusterig_window_size)\n",
    "    X_test_scaled = np.hstack([X_test_temp, X_test_humi])\n",
    "\n",
    "    X_test_scaled_each_device = X_test_scaled.reshape(-1, nb_devices, clusterig_window_size * 2)\n",
    "    feature_each_device = []\n",
    "    for device_id in range(0, nb_devices):\n",
    "        E_result = encoder.predict([X_test_scaled_each_device[:,device_id,:clusterig_window_size], X_test_scaled_each_device[:,device_id,clusterig_window_size:]])\n",
    "        feature_each_device.append(E_result)\n",
    "\n",
    "\n",
    "    feature_each_device = np.array(feature_each_device)\n",
    "    return feature_each_device\n",
    "\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "def find_best_nb_of_clustering (feature_each_device, test_data_index):\n",
    "    best_k = 0\n",
    "    c_start = 2\n",
    "    c_end = 8\n",
    "    c_inertia = []\n",
    "    c_sil_score = []\n",
    "    feature = feature_each_device[:,test_data_index,:]\n",
    "\n",
    "    for km_n_c in range(c_start,c_end):\n",
    "        pca = PCA(n_components = 2)\n",
    "        pca_result = pca.fit_transform(feature)\n",
    "        kmeans_model = KMeans(n_clusters = km_n_c)\n",
    "        inertia = kmeans_model.fit(pca_result).inertia_\n",
    "        c_inertia.append(inertia)\n",
    "\n",
    "    c_inertia = np.array(c_inertia)\n",
    "    best_k = np.argmax(np.diff(c_inertia, 2)) + 1 + c_start\n",
    "\n",
    "    return best_k\n",
    "\n",
    "##################################################################################################\n",
    "##################################################################################################\n",
    "def clustering(feature_each_device, km_n_c, test_data_index):\n",
    "    km_device_map = np.zeros((8,16)).tolist()\n",
    "    device_clustering_dict={}\n",
    "    for i in range(0, km_n_c):\n",
    "        device_clustering_dict[i]=[]\n",
    "    for i in range(0,8):\n",
    "        for j in range(0,16):\n",
    "            km_device_map[i][j]= ''\n",
    "\n",
    "    km_labels = []\n",
    "    data = feature_each_device[:,test_data_index,:]\n",
    "    # KMEAN\n",
    "    pca = PCA(n_components=2)\n",
    "    pca_result = pca.fit_transform(data)\n",
    "    km_labels.append(KMeans(n_clusters=km_n_c).fit_predict(pca_result))\n",
    "    km_labels = np.array(km_labels)\n",
    "    device_index=0\n",
    "    for key, val in device_location_map.items():\n",
    "        km_device_map[val[0]][val[1]] = str(km_labels[0, device_index])\n",
    "        device_clustering_dict[km_labels[0, device_index]].append(device_index)\n",
    "\n",
    "        device_index += 1\n",
    "    print(\"-------------------------------------------------\")\n",
    "    print_device_map(km_device_map)\n",
    "    print(\"-------------------------------------------------\")\n",
    "    print()\n",
    "    \n",
    "    return device_clustering_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a774d30",
   "metadata": {},
   "source": [
    "# Function for Training of Global LSTM model\n",
    "\n",
    "### Description\n",
    "- This function trains the Global LSTM model and saves best-trained model in the \"models\" folder.\n",
    "\n",
    "### 1. training_global_lstm_model()\n",
    "- Input:\n",
    "    - model_name : saved model name\n",
    "    - predict_size : number of time series data you want to predict, this paper set it to 10\n",
    "    - nb_epoch : number of epoch, this paper set it to 100\n",
    "    - batch_size : batch size of model, this paper set it to 32\n",
    "    - X_train : input for training data in training phase\n",
    "        - Shape : (:, 1, input_size)\n",
    "    - Y_train : output for training data in training phase\n",
    "        - Shape : (:, 1, predict_size)\n",
    "    - X_valid : input for validation data in training phase\n",
    "        - Shape : (:, 1, input_size)\n",
    "    - Y_valid : output for validation data in training phase\n",
    "        - Shape : (:, 1, predict_size)\n",
    "\n",
    "- Output:\n",
    "    - Save best trained model to \"models\" folder where filename is model_name+\"LSTM.h5\"\n",
    "\n",
    "\n",
    "### Example \n",
    "    training_global_lstm_model('temp_global_',predict_size, nb_epoch, batch_size, X_train_temp, Y_train_temp, X_valid_temp, Y_valid_temp)\n",
    "    training_global_lstm_model('humi_global_',predict_size, nb_epoch, batch_size, X_train_humi, Y_train_humi, X_valid_humi, Y_valid_humi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01958ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_global_lstm_model(model_name, predict_size, nb_epoch, batch_size, X_train, Y_train, X_valid, Y_valid):\n",
    "    input_layer= Input (shape = (1, input_size))\n",
    "\n",
    "    lstm_left_1 = LSTM(64, activation='relu', return_sequences=True)(input_layer)\n",
    "    lstm_left_2 = LSTM(64, activation='relu', return_sequences=True)(lstm_left_1)\n",
    "    # lstm_left_3 = LSTM(64, activation='relu', return_sequences=True)(lstm_left_2)\n",
    "    output_left = TimeDistributed(Dense(predict_size))(lstm_left_2)\n",
    "    lstm = Model (inputs = input_layer, outputs = output_left)\n",
    "\n",
    "    lstm.summary()\n",
    "    lstm.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])\n",
    "    plot_model(lstm, to_file='plot_model/lstm_model.png', show_shapes = True)\n",
    "\n",
    "    checkpointer = ModelCheckpoint(filepath=\"models/\"+model_name+\"LSTM.h5\", verbose=0, save_best_only=True)\n",
    "    tensorboard = TensorBoard(log_dir='./logs', histogram_freq=0, write_graph=True, write_images=True)\n",
    "    history = lstm.fit(X_train, Y_train,\n",
    "                    epochs=nb_epoch,\n",
    "                    batch_size=batch_size,\n",
    "                    shuffle=True,\n",
    "                    validation_data=(X_valid, Y_valid),\n",
    "                    verbose=1,\n",
    "                    callbacks=[checkpointer, tensorboard]).history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aba67bf",
   "metadata": {},
   "source": [
    "# Function for Training of Local LSTM model\n",
    "\n",
    "### Description\n",
    "- This function fine-tunes of Global LSTM model using clustering results.\n",
    "- This function receives the global_lstm model as an input argument and fix the weights of LSTM layers 1 and 2 so that they are not trained.\n",
    "- After that, the local LSTM model of each cluster is trained by adding layers that can learn the characteristics of each cluster.\n",
    "- Input and validation dataset are data of each cluster.\n",
    "\n",
    "### 1. training_local_lstm_model()\n",
    "- Input:\n",
    "    - predict_size : number of time series data you want to predict, this paper set it to 10\n",
    "    - nb_epoch_2 : epoch of Local LSTM model, this paper set it to 40\n",
    "    - batch_size : batch size of model, this paper set it to 32\n",
    "    - global_lstm : pretrained Global LSTM model\n",
    "    - input_layer : input layer of pretrained Global LSTM model\n",
    "    - output_left_1 : output layer of pretrained Global LSTM model\n",
    "    - c_idx : clustering index (if the number of clusters is 4, then it can be 0, 1, 2, 3)\n",
    "    - X_train_tl : input for training data in training phase\n",
    "        - Shape : (:, 1, input_size)\n",
    "    - Y_train_tl : output for training data in training phase\n",
    "        - Shape : (:, 1, predict_size)\n",
    "    - X_valid_tl : input for validation data in training phase\n",
    "        - Shape : (:, 1, input_size)\n",
    "    - Y_valid_tl : output for validation data in training phase\n",
    "        - Shape : (:, 1, predict_size)\n",
    "\n",
    "- Output\n",
    "    - Returns trained Local LSTM model \n",
    "\n",
    "\n",
    "### Example \n",
    "    temp_global_lstm = load_model(\"models/temp_global_LSTM.h5\", compile=False)\n",
    "    humi_global_lstm = load_model(\"models/humi_global_LSTM.h5\", compile=False)\n",
    "\n",
    "    temp_global_lstm.layers[1].name = 'lstm_temp_global_1'\n",
    "    temp_global_lstm.layers[2].name = 'lstm_temp_global_2'\n",
    "    temp_global_lstm.layers[3].name = 'time_distributed_temp_global_1'\n",
    "\n",
    "    humi_global_lstm.layers[1].name = 'lstm_humi_global_1'\n",
    "    humi_global_lstm.layers[2].name = 'lstm_humi_global_2'\n",
    "    humi_global_lstm.layers[3].name = 'time_distributed_humi_global_1'\n",
    "    \n",
    "    tl_model_temp = training_local_lstm_model(predict_size, nb_epoch_2, batch_size,\n",
    "                    humi_global_lstm, humi_global_lstm.input, humi_global_lstm.output, c_idx,\n",
    "                    X_train_tl, Y_train_tl, X_valid_tl, Y_valid_tl)\n",
    "    tl_model_humi = training_local_lstm_model(predict_size, nb_epoch_2, batch_size,\n",
    "                    humi_global_lstm, humi_global_lstm.input, humi_global_lstm.output, c_idx,\n",
    "                    X_train_tl, Y_train_tl, X_valid_tl, Y_valid_tl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43ecf3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_local_lstm_model(predict_size, nb_epoch_2, batch_size, global_lstm, input_layer, output_left_1, c_idx, X_train_tl, Y_train_tl, X_valid_tl, Y_valid_tl):\n",
    "    global_lstm.layers[1].trainable = False\n",
    "    global_lstm.layers[2].trainable = False\n",
    "\n",
    "    lstm_right_1 = LSTM(64, activation='relu', return_sequences=True)(input_layer)\n",
    "    lstm_right_2 = LSTM(64, activation='relu', return_sequences=True)(lstm_right_1)\n",
    "    # output_left  = TimeDistributed(Dense(predict_size))(lstm_left_2)\n",
    "    output_right = TimeDistributed(Dense(predict_size))(lstm_right_2)\n",
    "    merged = concatenate([output_left_1, output_right])\n",
    "    merged = Dense(32,activation = 'relu')(merged)\n",
    "    output_layer = Dense(predict_size, activation = 'linear')(merged)\n",
    "\n",
    "    tl_model = Model (inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "    opti = optimizers.Adam(lr=0.001)\n",
    "    tl_model.compile(optimizer=opti, loss='mean_squared_error', metrics=['accuracy'])\n",
    "    checkpointer = ModelCheckpoint(filepath=\"models/LSTM_final.h5\", verbose=0, save_best_only=True)\n",
    "    tensorboard = TensorBoard(log_dir='./logs', histogram_freq=0, write_graph=False, write_images=False)\n",
    "\n",
    "    history = tl_model.fit(X_train_tl[c_idx], Y_train_tl[c_idx],\n",
    "                        epochs=nb_epoch_2,\n",
    "                        batch_size=batch_size,\n",
    "                        shuffle=True,\n",
    "                        validation_data=(X_valid_tl[c_idx], Y_valid_tl[c_idx]),\n",
    "                        verbose=0,\n",
    "                        callbacks=[checkpointer, tensorboard]).history\n",
    "\n",
    "    return tl_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86deb8f6",
   "metadata": {},
   "source": [
    "# The device interval management algorithm applies the trained Local LSTM model to each device.\n",
    "- The transmission interval of each device is determined by predicting future data through the Local LSTM model and comparing it with the measured data.\n",
    "- For detailed algorithm, please refer to the algorithm in the paper."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
